{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze A/B Test Results\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "A/B tests are very commonly performed by data analysts and data scientists.\n",
    "\n",
    "For this project, I will try to understand the results of an A/B test run by an e-commerce website. My goal is to work through this notebook to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision.\n",
    "\n",
    "\n",
    "<a id='probability'></a>\n",
    "#### Part I - Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the 'ab_data.csv' data set into df variable\n",
    "df = pd.read_csv(\"ab_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows a top 5 rows as a sample from our data frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294478"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using shape function to get the structure of our dataframe\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using nunique function to get the number of te unique values in the specified column\n",
    "df.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12126269856564711"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the proportion of users who have value equal to 1 \n",
    "# in \"converted\" column\n",
    "user_conv_prop = df.query('converted == 1')[\"user_id\"].count() / df.user_id.nunique()\n",
    "\n",
    "# print user_conv_prop\n",
    "user_conv_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"treat_dnot_newp_num\" variable refers to how many times the treatment group\n",
    "# comes with the old_page\n",
    "treat_dnot_newp_num = df.query('group == \"treatment\" and landing_page != \"new_page\"').count()\n",
    "\n",
    "# \"newp_donot_treat_num\" variable refers to how many times the new_page\n",
    "# comes with the control group\n",
    "newp_donot_treat_num = df.query('group != \"treatment\" and landing_page == \"new_page\"').count()\n",
    "\n",
    "# total_num_treat_newp variable refers to the summation of above variables with each others\n",
    "total_num_treat_newp = treat_dnot_newp_num[0] + newp_donot_treat_num[0]\n",
    "\n",
    "# print total_num_treat_newp\n",
    "total_num_treat_newp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294478 entries, 0 to 294477\n",
      "Data columns (total 5 columns):\n",
      "user_id         294478 non-null int64\n",
      "timestamp       294478 non-null object\n",
      "group           294478 non-null object\n",
      "landing_page    294478 non-null object\n",
      "converted       294478 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Using info function to get detailed info about the columns \n",
    "# and how many values in each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rows where **treatment** is not aligned with **new_page** or **control** is not aligned with **old_page**, we cannot be sure if this row truly received the new or old page.\n",
    "\n",
    "Now I will remove those records and create new dataframe with the cleaned records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290585"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new data frames df2, subset_1, subset_2\n",
    "df2 = pd.DataFrame()\n",
    "subset_1 = pd.DataFrame()\n",
    "subset_2 = pd.DataFrame()\n",
    "\n",
    "# Subset_1 dataframe has all the rows with 'treatment' group and 'new_page' as landing_page\n",
    "subset_1 =df.query('group == \"treatment\" and landing_page == \"new_page\"')\n",
    "\n",
    "# Subset_2 dataframe has all the rows with 'control' group and 'old_page' as landing_page\n",
    "subset_2 =df.query('group == \"control\" and landing_page == \"old_page\"')\n",
    "\n",
    "# Merge subset_1 with subset_2 into df2\n",
    "df2= pd.merge(subset_1, subset_2, how=\"outer\")\n",
    "\n",
    "# Print the count of all records in df2\n",
    "df2.count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Double Check all of the correct rows were removed - this should be 0\n",
    "print(df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0])\n",
    "\n",
    "print(df2[((df2['group'] == 'control') == (df2['landing_page'] == 'old_page')) == False].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using nunique function to get the number of te unique values in the specified column\n",
    "df2.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1404    773192\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the repeated value of user_id in the newly created data frame df2\n",
    "df2[df2.duplicated(\"user_id\") == True].user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "938    773192  2017-01-09 05:37:58.781806  treatment     new_page          0\n",
       "1404   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the full row for user_id = 773192\n",
    "df2.query('user_id == \"773192\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id                   timestamp      group landing_page  converted\n",
       "938   773192  2017-01-09 05:37:58.781806  treatment     new_page          0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the second index of the duplicated index\n",
    "df2 = df2.drop_duplicates(subset=['user_id'], keep='first')\n",
    "\n",
    "# Double check that the last repeated user_id has been deleted \n",
    "df2.query('user_id == \"773192\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the probability of individuals who converted wheter to the old_page \n",
    "# or the new_page \n",
    "indiv_prob = df2.query('converted == \"1\"')[\"user_id\"].count() / df2.user_id.count()\n",
    "\n",
    "# print indiv_prob\n",
    "indiv_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the probability of being into control group and convert to the other page\n",
    "indiv_convert_control_prob = df2.query('group == \"control\" and converted == \"1\"').count() / df2.query('group == \"control\"')[\"user_id\"].count()\n",
    "\n",
    "# print indiv_convert_control_prob\n",
    "indiv_convert_control_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11880806551510564"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Calculating the probability of being into treatment group and convert to the other page\n",
    "indiv_convert_treat_prob = df2.query('group == \"treatment\" and converted == \"1\"').count() / df2.query('group == \"treatment\"')[\"user_id\"].count()\n",
    "\n",
    "# print indiv_convert_treat_prob\n",
    "indiv_convert_treat_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50006194422266881"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the probability of receiving new_page as landing page for individulas\n",
    "indiv_newp_prob = df2.query('landing_page == \"new_page\"')[\"user_id\"].count() / df2.user_id.count()\n",
    "\n",
    "# print indiv_newp_prob\n",
    "indiv_newp_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on the calculted conversion probailities of 'control' & 'treatment' groups, it seems that being a user in the 'control' group have a higher probability (0.00157) to convert to the other page.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "\n",
    "Notice that because of the time stamp associated with each event, we could technically run a hypothesis test continuously as each observation was observed.  \n",
    "\n",
    "However, I like to solve the hard question is do I stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time?  How long do I run to render a decision that neither page is better than another?  \n",
    "\n",
    "These questions are the difficult parts associated with A/B tests in general.  \n",
    "\n",
    "\n",
    "For now, We can consider that i need to make the decision just based on all the data provided.  I will assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H_0: p_{new} - p_{old} <= 0$$\n",
    "\n",
    "$$H_1: p_{new} - p_{old} > 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` I will assume under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the **converted** success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, I will assume they are equal to the **converted** rate in **ab_data.csv** regardless of the page. <br><br>\n",
    "\n",
    "I will use a sample size for each page equal to the ones in **ab_data.csv**.  <br><br>\n",
    "\n",
    "Performing the sampling distribution for the difference in **converted** between the two pages over 10,000 iterations of calculating an estimate from the null.  <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the convert_rate for the old_page\n",
    "p_new = df2.converted.mean()\n",
    "\n",
    "# print p_new\n",
    "p_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the convert_rate for the old_page\n",
    "p_old = df2.converted.mean()\n",
    "\n",
    "# print p_old\n",
    "p_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the proportion of new_page\n",
    "n_new = df2.query('landing_page == \"new_page\"').count()[0]\n",
    "\n",
    "# print n_old\n",
    "n_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145274"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Calculate the proportion of old_page\n",
    "n_old = df2.query('landing_page == \"old_page\"').count()[0]\n",
    "\n",
    "#print n_old\n",
    "n_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new array 'new_page_converted' has 0,1 based on our random samples\n",
    "new_page_converted = np.random.choice([0,1], size = n_new, p = [1-p_new, p_new])\n",
    "\n",
    "# print  new_page_converted\n",
    "new_page_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new array 'old_page_converted' has 0,1 based on our random samples\n",
    "old_page_converted = np.random.choice([0,1], size = n_old, p = [1-p_old, p_old])\n",
    "\n",
    "# print  old_page_converted\n",
    "old_page_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000245698993558\n"
     ]
    }
   ],
   "source": [
    "# Calculate the diff between the converted new/old means\n",
    "diff = new_page_converted.mean() - old_page_converted.mean()\n",
    "\n",
    "# print  diff\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Bootstraping Technique to calculate the diffs between our random samples\n",
    "p_diffs = []\n",
    "#for _ in range(10000):\n",
    "#    newp_convert_samp = np.random.choice([0,1], size = n_new, p = [1-p_new, p_new])\n",
    "#    oldp_convert_samp = np.random.choice([0,1], size = n_old, p = [1-p_old, p_old])\n",
    "#    diff = newp_convert_samp.mean() - oldp_convert_samp.mean()\n",
    "#    p_diffs.append(diff)\n",
    "\n",
    "# Using built-in numpy funtion (bionomial) is more efficient in time than using For loop\n",
    "new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new\n",
    "old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old\n",
    "p_diffs = new_converted_simulation - old_converted_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1599f59dd8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a histogram using the data calculated in the bootstrapping\n",
    "plt.hist(p_diffs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9058\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Null values using normal function\n",
    "# null_vals = np.random.normal(0, np.std(p_diffs), 10000)\n",
    "\n",
    "# Calculate the difference between the means of new/old pages\n",
    "real_diff = (df2.query('landing_page == \"new_page\"').converted.mean()) - (df2.query('landing_page == \"old_page\"').converted.mean())\n",
    "\n",
    "# Calculate p-value\n",
    "p_value = (p_diffs > real_diff).mean()\n",
    "\n",
    "# print p-value\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we just computed in the above cell which is equal to (0.9058) is called p-value and in scientific studies, this value is the probability of obtaining the observed statistic or one more extreme in favor of the alternative hypothesis if null hypothesis is true. In simple words, while p_value which is equal to 0.9058 and significance level is equal to 0.05 which will lead that the p_value > significance level , and this will lead us to \"fail to reject the null Hypothesis\" which assumed that the old_page is better than or equal to the new_page**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possible technique to calculate P-Value using Ztest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "# Calculate new values to be used in the upcoming Ztest\n",
    "convert_old = df2.query('landing_page == \"old_page\"').converted.mean()\n",
    "convert_new = df2.query('landing_page == \"new_page\"').converted.mean()\n",
    "n_old = df2.query('landing_page == \"old_page\"').count()[0]\n",
    "n_new = df2.query('landing_page == \"new_page\"').count()[0]\n",
    "success_num_conv_old = int(n_old * convert_old)\n",
    "success_num_conv_new = int(n_new * convert_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will use `stats.proportions_ztest` to compute my test statistic and p-value.  [Here](http://knowledgetack.com/python/statsmodels/proportions_ztest/) is a helpful link on using the built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.31092419842\n",
      "0.905058312759\n",
      "1.95996398454\n"
     ]
    }
   ],
   "source": [
    "# Calculate Ztest to get z_score and p-value\n",
    "# choosing 'smaller' because we start the test with belief that the Null Hypothesis\n",
    "# is the correct hypothesis until we have an evidance indicate something else\n",
    "z_score, p_value_2 = sm.stats.proportions_ztest([success_num_conv_old,success_num_conv_new], [n_old,n_new], alternative=\"smaller\")\n",
    "# print Z_score\n",
    "print(z_score)\n",
    "\n",
    "# import norm lib\n",
    "from scipy.stats import norm\n",
    "\n",
    "# print p-value \n",
    "print(norm.cdf(z_score))\n",
    "\n",
    "# The value to compared Z_score with to decide if we should accept/reject\n",
    "# the Null Hypothesis/ the Alternative Hypothesis\n",
    "critical_value = norm.ppf(1-(0.05/2))\n",
    "\n",
    "# print critical_value\n",
    "print(critical_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is the difference betwene the calculated p-value result from Ztest and the  p-value calculated from the bootstrapping Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z-score means how many standard deviations an element is from the mean. We can notice that the P_value_2 of (0.9) is the same with P_value of (0.9) which we get from the sampling distribution. Since the Z-score of (1.31092419842) do not exceeds the critical value of (1.95996398454), we fail to reject the Null Hypothesis between the two proportions. Thus, The findings of this cell are agreeing with the findings from Bootstrapping Technique.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part III - A regression approach\n",
    "\n",
    "In this final part, we will see that the result you acheived in the previous A/B test can also be acheived by performing regression.<br><br>\n",
    "\n",
    "##### I will The simple Linear Regression in this case. #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use **statsmodels** to fit the regression model I specified in part **a.** to see if there is a significant difference in conversion based on which page a customer receives.  However, firstly I need to create a column for the intercept, and create a dummy variable column for which page each user received. Also, I will create new column called as **ab_page** column, which is 1 when an individual receives the **treatment** and 0 if **control**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new two columns which are 'intercept' which is always 1\n",
    "# And the other column is 'ab_page' which is equal 1 in case of 'treatment' group \n",
    "# and is equal to 0 else\n",
    "df2['intercept'] = 1\n",
    "df2['ab_page'] = np.where(df2['group'] == \"treatment\", 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit new regression model \n",
    "l = sm.OLS(df2['converted'], df2[['intercept', 'ab_page']])\n",
    "results = l.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>converted</td>    <th>  R-squared:         </th> <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.719</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 11 Nov 2018</td> <th>  Prob (F-statistic):</th>  <td> 0.190</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:31:09</td>     <th>  Log-Likelihood:    </th> <td> -85267.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>290584</td>      <th>  AIC:               </th> <td>1.705e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>290582</td>      <th>  BIC:               </th> <td>1.706e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>    0.1204</td> <td>    0.001</td> <td>  141.407</td> <td> 0.000</td> <td>    0.119</td> <td>    0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0016</td> <td>    0.001</td> <td>   -1.311</td> <td> 0.190</td> <td>   -0.004</td> <td>    0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>125553.456</td> <th>  Durbin-Watson:     </th>  <td>   2.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>414313.355</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td> 2.345</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>        <td> 6.497</td>   <th>  Cond. No.          </th>  <td>    2.62</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   R-squared:                       0.000\n",
       "Model:                            OLS   Adj. R-squared:                  0.000\n",
       "Method:                 Least Squares   F-statistic:                     1.719\n",
       "Date:                Sun, 11 Nov 2018   Prob (F-statistic):              0.190\n",
       "Time:                        11:31:09   Log-Likelihood:                -85267.\n",
       "No. Observations:              290584   AIC:                         1.705e+05\n",
       "Df Residuals:                  290582   BIC:                         1.706e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept      0.1204      0.001    141.407      0.000       0.119       0.122\n",
       "ab_page       -0.0016      0.001     -1.311      0.190      -0.004       0.001\n",
       "==============================================================================\n",
       "Omnibus:                   125553.456   Durbin-Watson:                   2.000\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           414313.355\n",
       "Skew:                           2.345   Prob(JB):                         0.00\n",
       "Kurtosis:                       6.497   Cond. No.                         2.62\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the results of our regression model \n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. What is the p-value associated with **ab_page**? Why does it differ from the value you found in the **Part II**?<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The P_value associated with ab_page is equal to (0.190). Also, ab_page is not statistically significant because its p-value (0.190) is greater than the significance level (0.05)**\n",
    "\n",
    "**<br>The reason behind the different p_values in Part II and Part III is that the regression model assumes different alternative Hypothesis than the alternative Hypothesis used in the Ztest or Sampling Distribution**\n",
    "\n",
    "**In other words, the Sampling distribution and Ztest are checking if one page performs better than the other page. Controversly, the regression model checks if the two pages are performing exactly the same or differently**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will consider other things that might influence whether or not an individual converts. Why it is a good idea to consider other factors to add into my regression model.  Are there any disadvantages to adding additional terms into my regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Pros of using Multiple Linear Regression are two reasons. The first one is the ability to determine the relative influence of one/more predictor(s) to the criterion value. The second advantage is the ability to identify outliers/anomalies.**\n",
    "\n",
    "\n",
    "**The most noted disadvatage using Multiple Linear Regression is that the whole analysis can go seriously wrong if there are severe outliers or influential cases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now along with testing if the conversion rate changes for different pages, also add an effect based on which country a user lives. I will need to read in the **countries.csv** dataset and merge together your datasets on the approporiate rows.  [Here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html) are the docs for joining tables. \n",
    "\n",
    "##### Does it appear that country had an impact on conversion?.#####\n",
    "\n",
    "We will create dummy variables for these country columns. Also, Providing the statistical output as well as a written response to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>converted</td>    <th>  R-squared:         </th> <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.605</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 11 Nov 2018</td> <th>  Prob (F-statistic):</th>  <td> 0.201</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:00</td>     <th>  Log-Likelihood:    </th> <td> -85267.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>290584</td>      <th>  AIC:               </th> <td>1.705e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>290581</td>      <th>  BIC:               </th> <td>1.706e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>    0.1195</td> <td>    0.001</td> <td>  166.244</td> <td> 0.000</td> <td>    0.118</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>   -0.0042</td> <td>    0.003</td> <td>   -1.516</td> <td> 0.130</td> <td>   -0.010</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>        <td>    0.0010</td> <td>    0.001</td> <td>    0.746</td> <td> 0.455</td> <td>   -0.002</td> <td>    0.004</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>125552.384</td> <th>  Durbin-Watson:     </th>  <td>   2.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>414306.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td> 2.345</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>        <td> 6.497</td>   <th>  Cond. No.          </th>  <td>    4.84</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   R-squared:                       0.000\n",
       "Model:                            OLS   Adj. R-squared:                  0.000\n",
       "Method:                 Least Squares   F-statistic:                     1.605\n",
       "Date:                Sun, 11 Nov 2018   Prob (F-statistic):              0.201\n",
       "Time:                        11:34:00   Log-Likelihood:                -85267.\n",
       "No. Observations:              290584   AIC:                         1.705e+05\n",
       "Df Residuals:                  290581   BIC:                         1.706e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept      0.1195      0.001    166.244      0.000       0.118       0.121\n",
       "CA            -0.0042      0.003     -1.516      0.130      -0.010       0.001\n",
       "UK             0.0010      0.001      0.746      0.455      -0.002       0.004\n",
       "==============================================================================\n",
       "Omnibus:                   125552.384   Durbin-Watson:                   2.000\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           414306.036\n",
       "Skew:                           2.345   Prob(JB):                         0.00\n",
       "Kurtosis:                       6.497   Cond. No.                         4.84\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the 'countries.csv' file\n",
    "countries_df = pd.read_csv('countries.csv')\n",
    "countries_df.head()\n",
    "\n",
    "# Get the number of unqiue entries to know how many dummy column will be created\n",
    "countries_df.country.unique()\n",
    "\n",
    "merged_df = df2.join(countries_df.set_index('user_id'), on='user_id')\n",
    "\n",
    "# Double check on the merged columns\n",
    "merged_df.head()\n",
    "\n",
    "# Create new dummies columns\n",
    "merged_df[['CA','UK','US']] = pd.get_dummies(merged_df['country'])\n",
    "\n",
    "# Double check on the newly created dummies columns\n",
    "merged_df.head()\n",
    "\n",
    "# Create and fit the new regression model and get the results of it.\n",
    "lm = sm.OLS(merged_df['converted'], merged_df[['intercept','CA','UK']])\n",
    "results = lm.fit()\n",
    "results.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So my interpretation about the above regression model will be the follwoing:\n",
    "<br></br>\n",
    "<br> 1- The predicted converstion rate expected to be 11.95% regardless of the country. In other words, If the country was US, we are predicting the converstion rate to be 11.95%.</br>\n",
    "<br> 2- If the country is CA, then we are predicting the converstion rate to be 0.42% less than the conversation rate in the US. </br>\n",
    "<br> 3- If  the country is UK, then we are predicting the conversation rate to be increased by 0.10% rather than the conversation rate in the US. </br>** \n",
    "**<br> 4- Neither CA nor UK a statistically significant because their p-value (0.130),(0.455) respectively are greater than the significance level (0.05). </br>** \n",
    "**<br> 5- It is recommened to remove any non statisitcal signficant variable from our regression model to save it precision's level. </br>** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Though I have now looked at the individual factors of country and page on conversion, I would now like to look at an interaction between page and country to see if there significant effects on conversion. I will create the necessary additional columns, and fit the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>converted</td>    <th>  R-squared:         </th> <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2.477</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 11 Nov 2018</td> <th>  Prob (F-statistic):</th>  <td>0.0840</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:34:45</td>     <th>  Log-Likelihood:    </th> <td> -85266.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>290584</td>      <th>  AIC:               </th> <td>1.705e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>290581</td>      <th>  BIC:               </th> <td>1.706e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>  <td>    0.1196</td> <td>    0.001</td> <td>  183.238</td> <td> 0.000</td> <td>    0.118</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK_ab_page</th> <td>    0.0016</td> <td>    0.002</td> <td>    0.862</td> <td> 0.388</td> <td>   -0.002</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA_ab_page</th> <td>   -0.0077</td> <td>    0.004</td> <td>   -1.996</td> <td> 0.046</td> <td>   -0.015</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>125551.115</td> <th>  Durbin-Watson:     </th>  <td>   2.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>414297.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td> 2.345</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>        <td> 6.497</td>   <th>  Cond. No.          </th>  <td>    6.46</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   R-squared:                       0.000\n",
       "Model:                            OLS   Adj. R-squared:                  0.000\n",
       "Method:                 Least Squares   F-statistic:                     2.477\n",
       "Date:                Sun, 11 Nov 2018   Prob (F-statistic):             0.0840\n",
       "Time:                        11:34:45   Log-Likelihood:                -85266.\n",
       "No. Observations:              290584   AIC:                         1.705e+05\n",
       "Df Residuals:                  290581   BIC:                         1.706e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept      0.1196      0.001    183.238      0.000       0.118       0.121\n",
       "UK_ab_page     0.0016      0.002      0.862      0.388      -0.002       0.005\n",
       "CA_ab_page    -0.0077      0.004     -1.996      0.046      -0.015      -0.000\n",
       "==============================================================================\n",
       "Omnibus:                   125551.115   Durbin-Watson:                   2.000\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           414297.332\n",
       "Skew:                           2.345   Prob(JB):                         0.00\n",
       "Kurtosis:                       6.497   Cond. No.                         6.46\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new columns int eh merged data frame.\n",
    "# Those columns indicate the country and page effect together.\n",
    "# We will use them in our  next regression model.\n",
    "merged_df['US_ab_page'] = merged_df['US']*merged_df['ab_page']\n",
    "merged_df['UK_ab_page'] = merged_df['UK']*merged_df['ab_page']\n",
    "merged_df['CA_ab_page'] = merged_df['CA']*merged_df['ab_page']\n",
    "\n",
    "# Create and fit the new regression model and get the summary of it.\n",
    "lm = sm.OLS(merged_df['converted'], merged_df[['intercept','UK_ab_page','CA_ab_page']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My interpretation for the above regression model which look at an interaction between the page and the country will be the following:**\n",
    "\n",
    "**<br>1- The predicted converstion rate expected to be 11.95% regardless of the country and the page. In other words, If the country was US, we are predicting the converstion rate to be 11.95% regardless of the page</br>** \n",
    "**<br>2- If the country is UK, there is no statistical significant evidence betwene the conversion rate and the page/country, because p-value(0.388) is greater than the significance level (0.05)**\n",
    "**<br>3- If the country is CA, there is no statistical significant evidence betwene the conversion rate and the page/country, because p-value(0.046) is greater than the significance level (0.05)</br>.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
